{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:52.342511Z",
     "start_time": "2018-05-07T11:29:50.385716Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import ujson as json\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import stop_words\n",
    "from dateutil import parser\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "filenames = glob(\"bitcoin/*.json\")\n",
    "stopwords = set(stop_words.get_stop_words('en'))\n",
    "stopwords.update(['quote', 'pmquote', 'amquote', 'just', 'don', 'one', 'thing', 'even', 'way', 'maybe', 'also', 'please', 'well', 'actually', 'something',\n",
    "                                         'going', 'anything', 'le', 'ever', 'say', 'see', 'likely', 'per', 'another', 'someone', 'let', 'anyone', 'doesn', 'include', 'doe', 'exactly',\n",
    "                                         'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'like',\n",
    "                                         'said', 'guy', 'will', 'can', 'able', 'people', 'become', 'tell', 'hey', 'much', 'many', 'lol', 'lot', 'want', 'still', 'really', 'think', 'didn',\n",
    "                                         'isn', 'post', 'edited', 'share', 'facebookshare', 'twitter'])\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:29:54.235846Z",
     "start_time": "2018-05-07T11:29:54.225009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    input_string = re.sub(r'http\\S+', ' ', input_string)\n",
    "    input_string = re.sub(r'\\S+.(com|org)', '', input_string)\n",
    "    input_string = re.sub( \"[^a-zA-Z]\", \" \", input_string).split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in input_string]\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "    words = [w if w != 'bitcoins' else 'bitcoin' for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T11:37:28.909112Z",
     "start_time": "2018-05-07T11:33:15.478609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts_num'] = defaultdict(int)\n",
    "preprocessed_data['get_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['write_comment_num'] = defaultdict(int)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "for i, filename in enumerate(filenames):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for post in data:\n",
    "        post_body = parse_string(post['post_body'])\n",
    "        word_freq.update(post_body)\n",
    "        \n",
    "for i, filename in enumerate(filenames):\n",
    "    print(i)\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for post in data:        \n",
    "        post_body = parse_string(post['post_body'])\n",
    "        post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "        if len(post_body) < 5:\n",
    "            continue\n",
    "        voca.update(post_body)\n",
    "        post_user = post['post_user']\n",
    "        posted_time = parser.parse(post['posted_time']).date()\n",
    "        \n",
    "        preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "        preprocessed_data['user_posts_num'][post_user] += 1\n",
    "        preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "        if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        preprocessed_data['posts'].append(post_body)\n",
    "        \n",
    "        for comment in post['comments']:\n",
    "            comment_body = parse_string(comment['post_body'])\n",
    "            comment_body = [w for w in comment_body if word_freq[w] >= 10]\n",
    "            if len(comment_body) < 5:\n",
    "                continue\n",
    "            voca.update(comment_body)\n",
    "            comment_user = comment['post_user']\n",
    "            comment_time = parser.parse(comment['posted_time']).date()\n",
    "                \n",
    "            preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "            preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "            if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "                preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "            else:\n",
    "                preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "            \n",
    "            preprocessed_data['posts'].append(comment_body)\n",
    "            preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "            preprocessed_data['get_comment_num'][post_user] += 1\n",
    "            preprocessed_data['write_comment_num'][comment_user] += 1\n",
    "            \n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "with open(\"preprocessed_bitcoin.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T15:11:00.177641Z",
     "start_time": "2018-03-20T15:10:21.619147Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "    \n",
    "with open(\"etherium_forum.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for post in data:\n",
    "    post_body = parse_string(post['post_body'])\n",
    "    word_freq.update(post_body)\n",
    "\n",
    "for post in data:\n",
    "    post_body = parse_string(post['post_body'])\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    if len(post_body) < 5:\n",
    "        continue\n",
    "    voca.update(post_body)\n",
    "    post_user = post['post_user']\n",
    "    posted_time = parser.parse(post['posted_time']).date()\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    for comment in post['comments']:\n",
    "        comment_body = parse_string(comment['post_body'])\n",
    "        comment_body = [w for w in comment_body if word_freq[w] >= 10]\n",
    "        if len(comment_body) < 5:\n",
    "            continue\n",
    "        voca.update(comment_body)\n",
    "        comment_user = comment['post_user']\n",
    "        comment_time = parser.parse(comment['posted_time']).date()\n",
    "        \n",
    "        preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "        preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "        if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "        \n",
    "        preprocessed_data['posts'].append(comment_body)\n",
    "        preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "            \n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "with open(\"preprocessed_etherium.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T15:09:45.949318Z",
     "start_time": "2018-03-20T15:09:22.197366Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_posts = 0\n",
    "total_reple = 0\n",
    "preprocessed_data = {}\n",
    "preprocessed_data['user_network'] = nx.DiGraph()\n",
    "preprocessed_data['time_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_posts'] = defaultdict(list)\n",
    "preprocessed_data['user_time_posts'] = defaultdict(dict)\n",
    "preprocessed_data['posts'] = []\n",
    "voca = set()\n",
    "word_freq = Counter()\n",
    "    \n",
    "with open(\"ripple_forum.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for post in data:\n",
    "    post_body = parse_string(post['post_body'])\n",
    "    word_freq.update(post_body)\n",
    "\n",
    "for post in data:\n",
    "    post_body = parse_string(post['post_body'])\n",
    "    post_body = [w for w in post_body if word_freq[w] >= 10]\n",
    "    if len(post_body) < 5:\n",
    "        continue\n",
    "    voca.update(post_body)\n",
    "    post_user = post['post_user']\n",
    "    posted_time = parser.parse(post['posted_time']).date()\n",
    "    \n",
    "    preprocessed_data['user_posts'][post_user].append(post_body)\n",
    "    preprocessed_data['time_posts'][posted_time].append(post_body)\n",
    "    if posted_time in preprocessed_data['user_time_posts'][post_user]:\n",
    "            preprocessed_data['user_time_posts'][post_user][posted_time].append(post_body)\n",
    "    else:\n",
    "        preprocessed_data['user_time_posts'][post_user][posted_time] = [post_body]\n",
    "    preprocessed_data['posts'].append(post_body)\n",
    "\n",
    "    for comment in post['comments']:\n",
    "        comment_body = parse_string(comment['post_body'])\n",
    "        comment_body = [w for w in comment_body if word_freq[w] >= 10]\n",
    "        if len(comment_body) < 5:\n",
    "            continue\n",
    "        voca.update(comment_body)\n",
    "        comment_user = comment['post_user']\n",
    "        comment_time = parser.parse(comment['posted_time']).date()\n",
    "        \n",
    "        preprocessed_data['user_posts'][comment_user].append(comment_body)\n",
    "        preprocessed_data['time_posts'][comment_time].append(comment_body)\n",
    "        if posted_time in preprocessed_data['user_time_posts'][posted_time]:\n",
    "            preprocessed_data['user_time_posts'][posted_time][post_user].append(post_body)\n",
    "        else:\n",
    "            preprocessed_data['user_time_posts'][posted_time][post_user] = [post_body]\n",
    "        \n",
    "        preprocessed_data['posts'].append(comment_body)\n",
    "        preprocessed_data['user_network'].add_edge(comment_user, post_user)\n",
    "            \n",
    "voca = list(voca)\n",
    "preprocessed_data['voca'] = voca\n",
    "with open(\"preprocessed_ripple.pkl\", 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
